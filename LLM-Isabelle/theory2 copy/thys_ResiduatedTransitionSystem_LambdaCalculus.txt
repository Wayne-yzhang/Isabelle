    with reduction steps as non-identity transitions.  We represent both states and transitions
    in a unified, variable-free syntax based on de Bruijn indices.
    ``elementary reductions'', in which just one redex is contracted, are not preserved by
    residuation: an elementary reduction can have zero or more residuals along another
    elementary reduction.  However, ``parallel reductions'', which permit the contraction of
    multiple redexes existing in a term to be contracted in a single step, are preserved
    by residuation.  For this reason, in our syntax each term represents a parallel reduction
    of zero or more redexes; a parallel reduction of zero redexes representing an identity.
    This is a slightly different approach that that taken by other authors
    application constructor that is marked to indicate a redex to be contracted,
    but it seems more natural in the present setting in which a single syntax is used to
    represent both terms and reductions.

    Once the syntax has been defined, we define the residuation operation and prove
    that it satisfies the conditions for a weakly extensional RTS.  In this RTS, the source
    of a term is obtained by ``erasing'' the markings on redexes, leaving an identity term.
    The target of a term is the contractum of the parallel reduction it represents.
    As the definition of residuation involves the use of substitution, a necessary prerequisite
    is to develop the theory of substitution using de Bruijn indices.
    In addition, various properties concerning the commutation of residuation and substitution
    have to be proved.  This part of the work has benefited greatly from previous work
    in the proof assistant Coq.  In particular, it was very helpful to have already available
    known-correct statements of various lemmas regarding indices, substitution, and residuation.
    The development of the theory culminates in the proof of L\'{e}vy's ``Cube Lemma''

    we are able to take advantage of generic results already proved for RTS's; in particular,
    the construction of the RTS of paths, which represent reduction sequences.
    Very little additional effort is required at this point to prove the Church-Rosser Theorem.
    Then, after proving a series of miscellaneous lemmas about reduction paths,
    we turn to the study of developments.  A development of a term is a reduction path from
    that term in which the only redexes that are contracted are those that are residuals of
    redexes in the original term.  We prove the Finite Developments Theorem: all developments
    except that here we make the adaptations necessary for a syntax based on de Bruijn
    indices, rather than the classical named-variable syntax used by de Vrijer.
    Using the Finite Developments Theorem, we define a function that takes a term and constructs
    a ``complete development'' of that term, which is a development in which no residuals of
    original redexes remain to be contracted.

    We then turn our attention to ``standard reduction paths'', which are reduction paths in
    which redexes are contracted in a left-to-right order, perhaps with some skips.
    After giving a definition of standard reduction paths, we define a function that takes a
    term and constructs a complete development that is also standard.
    Using this function as a base case, we then define a function that takes an arbitrary
    parallel reduction path and transforms it into a standard reduction path that is congruent
    to the given path.  The algorithm used is roughly analogous to insertion sort.
    We use this function to prove strong form of the Standardization Theorem: every reduction
    path is congruent to a standard reduction path.  As a corollary of the Standardization
    Theorem, we prove the Leftmost Reduction Theorem: leftmost reduction is a normalizing
    reduction strategy.

    as well, but after encountering some unanticipated difficulties I decided not to attempt that
  \<close>
    text \<open>
      required for the residuation operation.
    \<close>
    text \<open>
      even though the former is a single constructor, whereas the latter contains two
      constructors.
    \<close>
    text \<open>
      The following function computes the set of free variables of a term.
      Note that since variables are represented by numeric indices, this is a set of numbers.
    \<close>
    text \<open>
      We will need to do some simultaneous inductions on pairs and triples of subterms
      of given terms.  We prove the well-foundedness of the associated relations using
      the following size measure.
    \<close>
    text \<open>
      Here we define some special classes of terms.
      It will be important for the commutation of substitution and residuation later on
      that substitution not be used in a way that could create any marked redexes;
    \<close>
    text \<open>
      For substitution, we need to be able to raise the indices of all free variables
      in a subterm by a specified amount.  To do this recursively, we need to keep track
      already greater than or equal to that depth, as these are the variables that are free
    \<close>
    text \<open>
      Ultimately, the definition of substitution will only directly involve the function
      that raises all indices of variables that are free in the outermost context;
      in a term, so we introduce an abbreviation for this special case.
    \<close>
    text \<open>
      The following development of the properties of raising indices, substitution, and
      In particular, it was very helpful to have correct statements of various lemmas
      available, rather than having to reconstruct them.
    \<close>
    text \<open>
      with adjustment of indices.  The ultimate goal is to define the result of contraction
    \<close>
    text \<open>
      but it has been left here as an aid to understanding.
    \<close>
    text \<open>
    \<close>
    text \<open>
      We now define residuation on terms.

      The definition ensures that, if residuation is defined for two terms, then those
      terms in must be arrows that are \emph{coinitial} (\emph{i.e.}~they are the same

    \<close>
    text \<open>
      Terms t and u are \emph{consistent} if residuation is defined for them.
    \<close>
    text \<open>
      A term can only be consistent with another if both terms are ``arrows''.
    \<close>
    text \<open>
      Residuation on consistent terms preserves arrows.
    \<close>
    text \<open>
      Here we give syntactic versions of the \emph{source} and \emph{target} of a term.
      These will later be shown to agree (on arrows) with the versions derived from the residuation.
      The underlying idea here is that a term stands for a reduction sequence in which
      in a bottom-up fashion.  A term without any marked redexes stands for an empty reduction
      sequence; such terms will be shown to be the identities derived from the residuation.
      The source of term is the identity obtained by erasing all markings; that is, by replacing
      identity that is the result of contracting all the marked redexes.
    \<close>
    text \<open>
      Two terms are syntactically \emph{coinitial} if they are arrows with the same source;
      that is, they represent two reductions from the same starting term.
    \<close>
    text \<open>
      We now show that terms are consistent if and only if they are coinitial.
    \<close>
    text \<open>
      The following result is not used in the formal development that follows,
      but it requires some proof and might eventually be useful.
    \<close>
              by (metis \<open>is_Lam t\<close> lambda.collapse(2) rev_image_eqI)
    text \<open>
      We now develop a series of lemmas that involve the interaction of residuation
      and substitution.
    \<close>
    text \<open>
      ``substitution commutes with residuation''.
    \<close>
    text \<open>
      for a residuated transition system.
    \<close>
    text \<open>
      We are now in a position to verify that the residuation operation that we have defined
      satisfies the axioms for a residuated transition system, and that various notions
      which we have defined syntactically above (\emph{e.g.}~arrow, source, target) agree
      with the versions derived abstractly from residuation.
    \<close>
    text \<open>
      The following classifies the ways that transitions can be sequential.  It is useful
      for later proofs by case analysis.
    \<close>
    text \<open>
      We ``almost'' have an extensional RTS.

      The following gives a concrete example of such a situation.
    \<close>
    text \<open>
      The following gives an example of two terms that are both coinitial and coterminal,
      but which are not congruent.
    \<close>
    text \<open>
      Every two coinitial transitions have a join, obtained structurally by unioning the sets
      of marked redexes.
    \<close>
                  using \<open>un_Lam t1 \ u1 \<noteq> \<^bold>\<sharp>\<close> t1u1 v by force
        by (metis \<open>prfx u (Join t u)\<close> arr_char assms cong_subst_right(2) prfx_implies_con
        by (metis Coinitial_resid_resid \<open>prfx t (Join t u)\<close> \<open>prfx u (Join t u)\<close> conE ide_char
        using \<open>(t \<squnion> u) \ t \<lesssim> u \ t\<close> cube by auto
        by (metis \<open>(t \<squnion> u) \ t \<lesssim> u \ t\<close> assms cube resid_Join)
    text \<open>
    \<close>  
    text \<open>
      The next two results are used to show that mapping App over lists of transitions
      preserves paths.
    \<close>
    text \<open>
      Here we define the usual relations of reduction and conversion.
    \<close>
  text \<open>
    We now define a locale that extends the residuation operation defined above
    to paths, using general results that have already been shown for paths in an RTS.
    In particular, we are taking advantage of the general proof of the Cube Lemma for
    residuation on paths.

    Our immediate goal is to prove the Church-Rosser theorem, so we first prove a lemma
    that connects the reduction relation to paths.  Later, we will prove many more
    facts in this locale, thereby developing a general framework for reasoning about
  \<close>
    text \<open>
      As a consequence of the Church-Rosser Theorem, the collection of all reduction
      paths forms a coherent normal sub-RTS of the RTS of reduction paths, and on identities
      the congruence induced by this normal sub-RTS coincides with convertibility.
      the only transitions are identities.
    \<close>
    interpretation Red: normal_sub_rts \<Lambda>x.Resid \<open>Collect \<Lambda>x.Arr\<close>
            \<open>\<And>a. \<Lambda>x.ide a \<Longrightarrow> a \<in> Collect \<Lambda>x.Arr\<close> mem_Collect_eq \<Lambda>x.arr_resid_iff_con)
    interpretation Red: coherent_normal_sub_rts \<Lambda>x.Resid \<open>Collect \<Lambda>x.Arr\<close>
    interpretation \<Lambda>q: quotient_by_coherent_normal \<Lambda>x.Resid \<open>Collect \<Lambda>x.Arr\<close>
    text \<open>
      A \emph{normal form} is an identity that is not the source of any non-identity arrow.
    \<close>
    text \<open>
      A term is \emph{normalizable} if it is an identity that is reducible to a normal form.
    \<close>
  text \<open>
  \<close>
      interpret Lam: simulation \<Lambda>.resid \<Lambda>.resid \<open>\<lambda>t. if \<Lambda>.arr t then \<^bold>\<lambda>\<^bold>[t\<^bold>] else \<^bold>\<sharp>\<close>
                  \<open>\<lambda>T. if Arr T then map (\<lambda>t. if \<Lambda>.arr t then \<^bold>\<lambda>\<^bold>[t\<^bold>] else \<^bold>\<sharp>) T else []\<close>
        by (simp add: \<open>map (\<lambda>t. if \<Lambda>.Arr t then \<^bold>\<lambda>\<^bold>[t\<^bold>] else \<^bold>\<sharp>) T = map \<Lambda>.Lam T\<close>)
      interpret App1: simulation \<Lambda>.resid \<Lambda>.resid \<open>\<lambda>t. if \<Lambda>.arr t then t \<^bold>\<circ> b else \<^bold>\<sharp>\<close>
                  \<open>\<lambda>T. if Arr T then map (\<lambda>t. if \<Lambda>.arr t then t \<^bold>\<circ> b else \<^bold>\<sharp>) T else []\<close>
      interpret App2: simulation \<Lambda>.resid \<Lambda>.resid \<open>\<lambda>u. if \<Lambda>.arr u then a \<^bold>\<circ> u else \<^bold>\<sharp>\<close>
                  \<open>\<lambda>T. if Arr T then map (\<lambda>u. if \<Lambda>.arr u then a \<^bold>\<circ> u else \<^bold>\<sharp>) T else []\<close>
    interpretation \<Lambda>\<^sub>L\<^sub>a\<^sub>m: sub_rts \<Lambda>.resid \<open>\<lambda>t. \<Lambda>.Arr t \<and> \<Lambda>.is_Lam t\<close>
                             \<open>\<lambda>t. if \<Lambda>\<^sub>L\<^sub>a\<^sub>m.arr t then \<Lambda>.un_Lam t else \<^bold>\<sharp>\<close>
    interpretation \<Lambda>\<^sub>A\<^sub>p\<^sub>p: sub_rts \<Lambda>.resid \<open>\<lambda>t. \<Lambda>.Arr t \<and> \<Lambda>.is_App t\<close>
                             \<open>\<lambda>t. if \<Lambda>\<^sub>A\<^sub>p\<^sub>p.arr t then \<Lambda>.un_App1 t else \<^bold>\<sharp>\<close>
                             \<open>\<lambda>t. if \<Lambda>\<^sub>A\<^sub>p\<^sub>p.arr t then \<Lambda>.un_App2 t else \<^bold>\<sharp>\<close>
                          \<open>\<lambda>T. if P\<^sub>A\<^sub>p\<^sub>p.Arr T then
                                 map (\<lambda>t. if \<Lambda>\<^sub>A\<^sub>p\<^sub>p.arr t then \<Lambda>.un_App1 t else \<^bold>\<sharp>) T
                               else []\<close>
                           \<open>\<lambda>T. if P\<^sub>A\<^sub>p\<^sub>p.Arr T then
                                  map (\<lambda>t. if \<Lambda>\<^sub>A\<^sub>p\<^sub>p.arr t then \<Lambda>.un_App2 t else \<^bold>\<sharp>) T
                                else []\<close>
      interpret Lam: simulation \<Lambda>.resid \<Lambda>.resid \<open>\<lambda>t. if \<Lambda>.arr t then \<^bold>\<lambda>\<^bold>[t\<^bold>] else \<^bold>\<sharp>\<close>
                        \<open>\<lambda>T. if Arr T then
                               map (\<lambda>t. if \<Lambda>.arr t then \<^bold>\<lambda>\<^bold>[t\<^bold>] else \<^bold>\<sharp>) T
                         else []\<close>
      interpret App: simulation \<Lambda>.resid \<Lambda>.resid \<open>\<lambda>t. if \<Lambda>.arr t then x \<^bold>\<circ> t else \<^bold>\<sharp>\<close>
                        \<open>\<lambda>T. if Arr T then map (\<lambda>t. if \<Lambda>.arr t then x \<^bold>\<circ> t else \<^bold>\<sharp>) T else []\<close>
      interpret App: simulation \<Lambda>.resid \<Lambda>.resid \<open>\<lambda>t. if \<Lambda>.arr t then t \<^bold>\<circ> x else \<^bold>\<sharp>\<close>
                        \<open>\<lambda>T. if Arr T then map (\<lambda>t. if \<Lambda>.arr t then t \<^bold>\<circ> x else \<^bold>\<sharp>) T else []\<close>
    text \<open>
      The following series of results is aimed at showing that a reduction path, all of whose
      into a reduction path in which only the ``rator'' components are reduced, followed
      by a reduction path in which only the ``rand'' components are reduced.
    \<close>
    text \<open>
      We arrive at the final objective of this section: factorization, up to congruence,
      into the composite of a path that reduces only the ``rators'' and a path
      that reduces only the ``rands''.
    \<close>
    text \<open>
      up to congruence, either as a path in which the top-level redex is
      contracted first, or as a path in which the top-level redex is contracted last.
    \<close>
    text \<open>
    \<close>
        by (metis Arr_map_un_Lam \<open>seq [\<^bold>\<lambda>\<^bold>[t\<^bold>]] U\<close> \<Lambda>.lambda.discI(2) mem_Collect_eq
  text \<open>
    A \emph{development} is a reduction path from a term in which at each step exactly one
    redex is contracted, and the only redexes that are contracted are those that are residuals
    of redexes present in the original term.  That is, no redexes are contracted that were
    newly created as a result of the previous reductions.  The main theorem about developments
    is the Finite Developments Theorem, which states that all developments are finite.
    the definitive work because de Vrijer's proof gives an exact bound on the number of steps
    in a development.  Since de Vrijer used a classical, named-variable representation of
    correct way to adapt de Vrijer's proof to the de Bruijn index representation of terms.
    I found this to be a somewhat delicate matter and to my knowledge it has not been done
    previously.
  \<close>
    text \<open>
      We define an \emph{elementary reduction} defined to be a term with exactly one marked redex.
      These correspond to the most basic computational steps.
    \<close>
    text \<open>
      It is tempting to imagine that elementary reductions would be atoms with respect to the
      nor is it elementary.
    \<close>
    text \<open>
      An \emph{elementary reduction path} is a path in which each step is an elementary reduction.
      It will be convenient to regard the empty list as an elementary reduction path, even though
      it is not actually a path according to our previous definition of that notion.
    \<close>
    text \<open>
      In the formal definition of ``development'' given below, we represent a set of
      Intuitively, such developments should consist of a (possibly empty) initial segment
      after what has come so far.
      between their sets of marked redexes.  In particular, this can occur when
      So we need to introduce a notion of containment between terms that is stronger
    \<close>
    text \<open>
    \<close>
    text \<open>
    \<close>
    text \<open>
    \<close>
    text \<open>
      to prove the Finite Developments Theorem: every term has the finite developments
      property.
    \<close>
  text \<open>
    a bound on the length of a development of a term.
    occur before the contraction of the top-level redex.  The development first

    In the general case, the problem is that reductions of residuals of t can
    them at any particular stage.  Hindley shows that developments in which
    top-level redex are equivalent to reductions of the special form, by a
    transformation with a bounded increase in length.  This can be considered as a
    weak form of standardization for developments.

    exact number of steps in a development of maximal length.  His proof is very
    straightforward and amenable to formalization, and it is what we follow here.
    This means that we have to discover the correct modification of de Vrijer's definitions
    to apply to the present situation.
  \<close>
    text \<open>
      Our first definition is that of the ``multiplicity'' of a free variable in a term.
      This is a count of the maximum number of times a variable could occur free in a term
      reachable in a development.  The main issue in adjusting to de Bruijn indices
      is that the same variable will have different indices depending on the depth at which
      it occurs in the term.  So, we need to keep track of how the indices of variables change
      as we move through the term.  Our modified definitions adjust the parameter to the
      multiplicity function on each recursive call, to account for the contextual depth
      (\emph{i.e.}~the number of binders on a path from the root of the term).
     
      The definition of this function is readily understandable, except perhaps for the
      This leads to the relation:
      \begin{center}
      \end{center}
      proving the termination is problematic.  Instead, we have to guess the correct
    
      These considerations lead to the following:
      \begin{center}
      \end{center}
      However, we can simplify this to:
      \begin{center}
      \end{center}
      for the termination proof and allow it to be done automatically.
     
      The final result is perhaps about the first thing one would think to write down,
      but there are possible ways to go wrong and it is of course still necessary to discover
      the proper form required for the various induction proofs.  I followed a long path
      of rather more complicated-looking definitions, until I eventually managed to find the
      proper inductive forms for all the lemmas and eventually arrive back at this definition.
    \<close>
    text \<open>
      The multiplicity function generalizes the free variable predicate.
      This is not actually used, but is included for explanatory purposes.
    \<close>
    text \<open>
      We now establish a fact about commutation of multiplicity and Raise that will be
      needed subsequently.
    \<close>
    text \<open>
      We can now (!) prove the desired generalization of de Vrijer's formula for the
      commutation of multiplicity and substitution.  This is the main lemma whose form
      is difficult to find.  To get this right, the proper relationships have to exist
    \<close>
    text \<open>
    \<close>
    text \<open>
      Next we define the ``height'' of a term.  This counts the number of steps in a development
      of maximal length of the given term.
    \<close>
                by (metis "1" Suc_le_mono \<open>mtp 0 (t1 \ u1') \<le> mtp 0 t1\<close> add_less_le_mono
    text \<open>
      We finally arrive at the main result of this section:
      the Finite Developments Theorem.
    \<close>
    text \<open>
      A \emph{complete development} is a development in which there are no residuals of originally
      marked redexes left to contract.
    \<close>
    text \<open>
      Now that we know all developments are finite, it is easy to construct a complete development
      by an iterative process that at each stage contracts one of the remaining marked redexes
      at each stage.  It is also possible to construct a complete development by structural
      induction without using the finite developments property, but it is more work to prove the
      correctness.
    \<close>
    text \<open>
      A \emph{reduction strategy} is a function taking an identity term to an arrow having that
      identity as its source.
    \<close>
    text \<open>
      The following defines the iterated application of a reduction strategy to an identity term.
    \<close>
    text \<open>
      A reduction strategy is \emph{normalizing} if iterated application of it to a normalizable
      term eventually yields a normal form.
    \<close>
    text \<open>
      The following function constructs the reduction path that results by iterating the
      application of a reduction strategy to a term.
    \<close>
    text \<open>
       \emph{Parallel reduction} is the strategy that contracts all available redexes at each step.
    \<close>
    text \<open>
     Parallel reduction is a universal strategy in the sense that every reduction path is
    \<close>
    text \<open>
      Parallel reduction is a normalizing strategy.
    \<close>
    text \<open>
      An alternative characterization of a normal form is a term on which the parallel
      reduction strategy yields an identity.
    \<close>
    text \<open>
      \emph{Head reduction} is the strategy that only contracts a redex at the ``head'' position,
      which is found at the end of the ``left spine'' of applications, and does nothing if there is
      no such redex.

    \<close>
    text \<open>
      The following function tests whether a term is an elementary reduction of the head redex.
    \<close>
    text \<open>
      The following function tests whether a redex in the head position of a term is marked.
    \<close>
    text \<open>
      An \emph{internal reduction} is one that does not contract any redex at the head position.
    \<close>
    text \<open>
      arbitrary reductions.
    \<close>
            by (metis "0" ConI Con_implies_is_Lam_iff_is_Lam \<open>Arr (t1 \<^bold>\<circ> t2)\<close>
            using Arr_resid \<open>Arr (t1 \<^bold>\<circ> t2)\<close> tu u by auto
                            \<open>Arr (t1 \<^bold>\<circ> t2)\<close> Coinitial_iff_Con
    text \<open>
       Internal reductions are closed under residuation.
    \<close>
    text \<open>
      A head reduction is preserved by residuation along an internal reduction,
      so a head reduction can only be canceled by a transition that contains a head reduction.
    \<close>
          using t u u1u2 1 Coinitial_iff_Con \<open>Con t2 u2\<close> ide_char ind1 resid_Ide_Arr
    text \<open>
    \<close>
    text \<open>
      An internal reduction cannot create a new head redex.
    \<close>
    text \<open>
      Residuation along internal reductions preserves head reductions.
    \<close>
    text \<open>
      An internal reduction followed by a head reduction can be expressed
      as a join of the internal reduction with a head reduction.
    \<close>
    text \<open>
      Leftmost (or normal-order) reduction is the strategy that produces an elementary
      reduction path by contracting the leftmost redex at each step.  It agrees with
      head reduction as long as there is a head redex, otherwise it continues on with the next
      subterm to the right.
    \<close>
    text \<open>
      In this section, we define the notion of a \emph{standard reduction}, which is an
      elementary reduction path that performs reductions from left to right, possibly
      skipping some redexes that could be contracted.  Once a redex has been skipped,
      neither that redex nor any redex to its left will subsequently be contracted.
      We then define and prove correct a function that transforms an arbitrary
      elementary reduction path into a congruent standard reduction path.
      Using this function, we prove the Standardization Theorem, which says that
      every elementary reduction path is congruent to a standard reduction path.
      We then show that a standard reduction path that reaches a normal form is in
      fact a leftmost reduction path.  From this fact and the Standardization Theorem
      we prove the Leftmost Reduction Theorem: leftmost reduction is a normalizing
      strategy.

      with subsequent proofs given by a number of authors.  Formalized proofs have also
      to earlier work.  The version of the theorem that we formalize here is a ``strong''
      version, which asserts the existence of a standard reduction path congruent to a
      a given elementary reduction path.  At the core of the proof is a function that
      directly transforms a given reduction path into a standard one, using an algorithm
      roughly analogous to insertion sort.  The Finite Development Theorem is used in the
      proof of termination.  The proof of correctness is long, due to the number of cases that
      have to be considered, but the use of a proof assistant makes this manageable.
    \<close>
    text \<open>
      We first need to define the notion of a ``standard reduction''.  In contrast to what
      is typically done by other authors, we define this notion by direct comparison of adjacent
      terms in an elementary reduction path, rather than by using devices such as a numbering
      of subterms from left to right.

      captures what we intend.  Most of the clauses are readily understandable.
      One clause that perhaps could use some explanation is the one for
    \<close>
    text \<open>
      A head reduction is standardly sequential with any elementary reduction that
      can be performed after it.
    \<close>
    text \<open>
      Once a head reduction is skipped in an application, then all terms that follow it
      in a standard reduction path are also applications that do not contain head reductions.
    \<close>
    text \<open>
      A \emph{standard reduction path} is an elementary reduction path in which
      successive reductions are standardly sequential.
    \<close>
    text \<open>
      Given a standard reduction path, all of whose transitions have App as their top-level
      onto paths formed from the ``rator'' and the ``rand'' of each application.  These projected
      paths are not standard, since the projection operation will introduce identities,
      in general.  However, in this section we show that if we remove the identities, then
      in fact we do obtain standard reduction paths.
    \<close>
                by (metis 1 Std.simps(2-3) \<open>U \<noteq> []\<close> ind list.exhaust_sel list.sel(1)
                by (metis "1" Std.simps(2) Std.simps(3) \<open>U \<noteq> []\<close> ind list.exhaust_sel list.sel(1)
    text \<open>
      If the first step in a standard reduction path contracts a redex that is
      top-level operator.
    \<close>
          using Std ind [of u] \<open>set U \<subseteq> Collect \<Lambda>.elementary_reduction\<close> by simp
    text \<open>
      Finite Development Theorem.
    \<close>
                  using \<open>\<Lambda>.con (\<^bold>\<lambda>\<^bold>[\<Lambda>.Src t\<^bold>] \<^bold>\<Zspot> \<Lambda>.Src u) (\<^bold>\<lambda>\<^bold>[t\<^bold>] \<^bold>\<Zspot> u)\<close> \<Lambda>.con_sym by blast
                  using \<open>\<Lambda>.con (\<^bold>\<lambda>\<^bold>[\<Lambda>.Src t\<^bold>] \<^bold>\<Zspot> \<Lambda>.Src u) (\<^bold>\<lambda>\<^bold>[t\<^bold>] \<^bold>\<Zspot> u)\<close> by blast
    text \<open>
      In this section, we define and prove correct a function that takes an arbitrary
      reduction path and produces a standard reduction path congruent to it.
      The method is roughly analogous to insertion sort: given a path, recursively
      standardize the tail and then ``insert'' the head into to the result.
      A complication is that in general the head may be a parallel reduction instead
      of an elementary reduction, and in any case elementary reductions are
      not preserved under residuation so we need to be able to handle the parallel
      reductions that arise from permuting elementary reductions.
      In general, this means that parallel reduction steps have to be decomposed into factors,
      and then each factor has to be inserted at its proper position.
      Another issue is that reductions don't all happen at the top level of a term,
      so we need to be able to descend recursively into terms during the insertion
      procedure.  The key idea here is: in a standard reduction, once a step has occurred
      top-level constructor.  So, once we have passed a step that is not a head reduction,
      we can recursively descend into the subsequent applications and treat the ``rator''
      and the ``rand'' parts independently.

      The following function performs the core insertion part of the standardization
      path.  This is so that it is possible to decide when the rest of the steps will be
      applications and it is therefore possible to recurse into them.  This constrains what
      recursive calls we can make, since we are not able to make a recursive call in which
      hypotheses in the correctness proof, we need to make sure that we don't make
      projected to their ``rator'' and ``rand'' components, which are treated separately
      and the results concatenated.  However, the projection operations can introduce
      identities and therefore do not preserve elementary reductions.  To handle this,
      we need to filter out identities after projection but before the recursive call.

      Ensuring termination also involves some care: we make recursive calls in which
      the length of the second argument is increased, but the ``height'' of the first
      argument is decreased.  So we use a lexicographic order that makes the height
      of the first argument more significant and the length of the second argument
      secondary.  The base cases either discard paths that consist entirely of
      development.
    \<close>
          using Std \<open>v = \<^bold>\<lambda>\<^bold>[a\<^bold>] \<^bold>\<Zspot> b\<close> Std.elims(2) \<Lambda>.sseq_Beta
    text \<open>
      leaves such a term at the head of the result.  Stated another way,
    \<close>
    text \<open>
      This is the correctness lemma for insertion:
      the result of insertion is a standard reduction path which is

      The proof is very long.  Its structure parallels that of the definition
      I strongly suggest viewing the proof in Isabelle/JEdit and using the
      code folding feature to unfold the proof a little bit at a time.
    \<close>
                    by (simp add: \<open>Arr U\<close> arr_char prfx_reflexive)
                          Std Std_consE Std_imp_Arr U \<open>Std (stdz_insert (\<Lambda>.Beta M N) (u # U))\<close>
      text \<open>
        Because of the way the function package processes the pattern matching in the
        of the proof, even though these subgoals are all simple consequences of a single,
        more general fact.  We first prove this fact, then use it to discharge the eight
        subgoals.
      \<close>
                            \<open>Std (stdz_insert (M \<^bold>\<circ> N) (u # U))\<close>
                            \<open>Std (stdz_insert (M \<^bold>\<circ> N) (u # U))\<close>
                            \<open>Std (stdz_insert (M \<^bold>\<circ> N) (u # U))\<close> arrI\<^sub>P arr_append_imp_seq
                    using 1 2 3 4 5 * ** \<open>\<Lambda>.is_App u \<or> \<Lambda>.is_Beta u\<close>
                        using U \<open>U \<noteq> [] \<Longrightarrow> Arr U\<close> by presburger
                              prfx_transitive \<open>U \<noteq> [] \<Longrightarrow> Arr U\<close>)
                      using 1 2 3 4 U * ** \<open>\<Lambda>.is_App u \<or> \<Lambda>.is_Beta u\<close>
                                  \<open>Std (stdz_insert (M \<^bold>\<circ> N) (u # U))\<close>
                      using 1 2 3 4 U * ** \<open>\<Lambda>.is_App u \<or> \<Lambda>.is_Beta u\<close>
                          using U \<open>U \<noteq> [] \<Longrightarrow> Arr U\<close> by blast
                                \<open>Std (stdz_insert (M \<^bold>\<circ> N) (u # U))\<close> arr_append_imp_seq
                  using "3" \<open>\<Lambda>.is_App u \<or> \<Lambda>.is_Beta u\<close> \<Lambda>.is_Beta_def u by force
                    using MN 1 2 3 5 * ** \<open>\<Lambda>.is_App u \<or> \<Lambda>.is_Beta u\<close>
                                  using u \<open>\<Lambda>.seq (M \<^bold>\<circ> N) u\<close> \<Lambda>.seq_char \<Lambda>.is_App_def by auto
                              using M MN \<open>\<Lambda>.Trg (\<Lambda>.un_App1 (last (u # U))) = \<Lambda>.Trg M\<close>
                                    \<open>Std (stdz_insert (M \<^bold>\<circ> N) (u # U))\<close> arr_append_imp_seq
                                    using MN \<open>\<Lambda>.App (\<Lambda>.Trg M) (\<Lambda>.Trg N) = \<Lambda>.Src u\<close> u by auto
                                          \<open>filter notIde (map \<Lambda>.un_App1 (u # U)) = []\<close>
                                by (meson \<open>seq [M \<^bold>\<circ> \<Lambda>.Src N] [\<Lambda>.Trg M \<^bold>\<circ> N]\<close> cong_reflexive seqE)
                                          \<open>Arr (u # U)\<close> \<open>set (u # U) \<subseteq> Collect \<Lambda>.is_App\<close>
                                        \<open>Std (filter notIde (map \<Lambda>.un_App1 (u # U)))\<close>
                                          \<open>Arr (u # U)\<close> \<open>set (u # U) \<subseteq> Collect \<Lambda>.is_App\<close>
                                    by (metis \<open>\<not> Ide (map \<Lambda>.un_App1 (u # U))\<close>
                                      \<open>[\<Lambda>.Trg (\<Lambda>.un_App1 (last (u # U))) \<^bold>\<circ> N] \<^sup>*\<sim>\<^sup>*
                                       [\<Lambda>.Trg (\<Lambda>.un_App1 (last (u # U))) \<^bold>\<circ> N]\<close>
                                      \<open>[\<Lambda>.Trg (\<Lambda>.un_App1 (last (u # U))) \<^bold>\<circ> N] \<^sup>*\<sim>\<^sup>*
                                       [\<Lambda>.Trg (\<Lambda>.un_App1 (last (u # U))) \<^bold>\<circ> N]\<close> ide_char)
                                          \<open>Arr (u # U)\<close> \<open>set (u # U) \<subseteq> Collect \<Lambda>.is_App\<close>
                                  Resid.simps(1) Std_imp_Arr \<open>Std (stdz_insert (M \<^bold>\<circ> N) (u # U))\<close>
                                          \<open>U \<noteq> [] \<Longrightarrow> Arr U\<close> \<Lambda>.arr_char last_map)
                                        \<open>set ([u] @ U) \<subseteq> Collect \<Lambda>.is_App\<close> append_Cons
                                          \<open>\<Lambda>.Ide (\<Lambda>.Src (\<Lambda>.un_App2 (hd ([u] @ U))))\<close>
      text \<open>
        Unfortunately, I haven't found a way to discharge them without having to state each
        one of them explicitly.
      \<close>
    text \<open>
      There is still a little bit more work to do, because we have to deal with various
      cases in which the reduction path to be standardized is empty or consists
      entirely of identities.
    \<close>
              by (metis Src_hd_eqI Trg_last_Src_hd_eqI \<open>T \<noteq> []\<close> append_Cons arrI\<^sub>P
                  using T \<open>Arr T\<close> ind by blast
    text \<open>
      In this section we prove the Leftmost Reduction Theorem, which states that
      leftmost reduction is a normalizing strategy.

      We first show that if a standard reduction path reaches a normal form,
      then the path must be the one produced by following the leftmost reduction strategy.
      This is because, in a standard reduction path, once a leftmost redex is skipped,
      all subsequent reductions occur ``to the right of it'', hence they are all non-leftmost
      reductions that do not contract the skipped redex, which remains in the leftmost position.

      The Leftmost Reduction Theorem then follows from the Standardization Theorem.
      If a term is normalizable, there is a reduction path from that term to a normal form.
      By the Standardization Theorem we may as well assume that path is standard.
      But a standard reduction path to a normal form is the path generated by following
      the leftmost reduction strategy, hence leftmost reduction reaches a normal form after
      a finite number of steps.
    \<close>
          using Std \<open>\<Lambda>.NF (Trg (t # u # T))\<close> ind sseq_reflects_leftmost_reduction by auto
